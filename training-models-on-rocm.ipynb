{
"cells": [
{
"cell_type": "markdown",
"metadata": {
"id": "NQUk3Y0WwYZ4"
},
"source": [
"# Train Models Using LeRobot on MI300x\n",
"\n",
"This guide walks you through setting up environment for training imitation learning policies using LeRobot library on a DigitalOcean (DO) instance equipped with AMD MI300x GPUs and ROCm.\n",
"\n",
"## ⚙️ Requirements\n",
"- A Hugging Face dataset repo ID containing your training data (`--dataset.repo_id=${HF_USER}/${DATASET_NAME}`).\n",
" If you don’t have an access token yet, you can sign up for Hugging Face [here](https://huggingface.co/join). After signing up, create an access token by visiting [here](https://huggingface.co/settings/tokens).\n",
"- A wandb account to enable training visualization and upload your training evidence to our github.\n",
" You can sign up for Wandb [here](https://wandb.ai/signup) and visit [here](https://wandb.ai/authorize) to create a token.\n",
"- Access to DO instance AMD Mi300x GPU\n"
]
},
{
"cell_type": "markdown",
"metadata": {
"id": "MOJyX0CnwA5m"
},
"source": [
"## Verify ROCm and GPU availability\n",
"This cell uses `pytorch` to check AMD GPU Info. The expected ouput is \n",
"`\n",
    "CUDA compatible device availability: True\n",
    "device name [0]: AMD Instinct MI300X VF\n",
    "`"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"import torch\n",
"print(f'CUDA compatible device availability:',torch.cuda.is_available())\n",
"print(f'device name [0]:', torch.cuda.get_device_name(0))\n"
]
},
{
"cell_type": "markdown",
"metadata": {
"id": "MOJyX0CnwA5m"
},
"source": [
"## Install FFmpeg 7.x\n",
"This cell uses `apt` to install ffmpeg 7.x for LeRobot."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "QlKjL1X5t_zM",
"outputId": "3b375aa1-19ed-4811-ff76-0afd5b762992"
},
"outputs": [],
"source": [
"!add-apt-repository ppa:ubuntuhandbook1/ffmpeg7 -y # install PPA which contains ffmpeg 7.x\n",
"!apt update && apt install ffmpeg -y"
]
},
{
"cell_type": "markdown",
"metadata": {
"id": "DxCc3CARwUjN"
},
"source": [
"## Install LeRobot v0.4.1\n",
"This cell clones the `lerobot` repository from Hugging Face, and installs the package in editable mode. Extra Features: To install additional dependencies for training SmolVLA or Pi models, refer to the [LeRobot offical page](https://huggingface.co/docs/lerobot/index). \n"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "dgLu7QT5tUik",
"outputId": "e46913b8-1977-48a5-a851-d8c69602419a"
},
"outputs": [],
"source": [
"!git clone https://github.com/huggingface/lerobot.git\n",
"!cd lerobot && git checkout -b v0.4.1 v0.4.1 # let’s synchronize using this version\n",
"!cd lerobot && pip install -e ."
]
},
{
"cell_type": "markdown",
"metadata": {
"id": "Q8Sn2wG4wldo"
},
"source": [
"## Weights & Biases login\n",
"This cell install and log into Weights & Biases (wandb) to enable experiment tracking and logging."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "PolVM_movEvp",
"outputId": "1769c6bd-8644-4b65-84c7-4f020b234c92"
},
"outputs": [],
"source": [
"!pip install wandb\n",
"import wandb\n",
"wandb.login(key=\"your_wandb_token\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Login into Hugging Face Hub"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "PolVM_movEvp",
"outputId": "1769c6bd-8644-4b65-84c7-4f020b234c92"
},
"outputs": [],
"source": [
"from huggingface_hub import login\n",
"login(token=\"your_huggingface_token\")"
]
},
{
"cell_type": "markdown",
"metadata": {
"id": "IkzTo4mNwxaC"
},
"source": [
"## Start Training Models with LeRobot\n",
"\n",
"This cell uses the lerobot-train CLI from the lerobot library to train a robot control policy. \n",
"\n",
"Make sure to adjust the following arguments to your setup:\n",
"\n",
"1. `--dataset.repo_id=YOUR_HF_USERNAME/YOUR_DATASET`: \n",
" Replace this with the Hugging Face Hub repo ID where your dataset is stored, e.g., `lerobot/svla_so100_pickplace`.\n",
"\n",
"2. `--policy.type=act`: \n",
" Specifies the policy configuration to use. `act` refers to [configuration_act.py](../lerobot/common/policies/act/configuration_act.py), which will automatically adapt to your dataset’s setup (e.g., number of motors and cameras).\n",
"\n",
"3. `--output_dir=outputs/train/...`: \n",
" Directory where training logs and model checkpoints will be saved.\n",
"\n",
"4. `--job_name=...`: \n",
" A name for this training job, used for logging and Weights & Biases.The name typically includes the model type (e.g., act, smolvla), the dataset name, and additional descriptive tags.\n",
"\n",
"5. `--policy.device=cuda`: \n",
" Use `cuda` if training on an AMD or NVIDIA GPU. \n",
"\n",
"6. `--wandb.enable=true`: \n",
" Enables Weights & Biases for visualizing training progress. You must be logged in via `wandb login` before running this.\n",
"\n",
"7. `--policy.push_to_hub=`:\n",
"\n",
" Enables automatic uploading of the trained policy to the Hugging Face Hub. You must specify `--policy.repo_id` (e.g., ${HF_USER}/{REPO_NAME}) if it is True."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "Ufss6US6xbpi",
"outputId": "cfd494c7-8689-419d-bb17-7f02a67b26d6"
},
"outputs": [],
"source": [
"!lerobot-train \\\n",
" --dataset.repo_id=ichbinblau/so101_stack2cubes_dataset \\\n",
" --batch_size=64 \\\n",
" --steps=1000 \\\n",
" --output_dir=outputs/train/act_so101_3cube_1ksteps \\\n",
" --job_name=act_so101_3cube_1ksteps \\\n",
" --policy.repo_id=ichbinblau/so101_act_stack2cubes \\\n",
" --policy.device=cuda \\\n",
" --policy.type=act \\\n",
" --policy.push_to_hub=true \\\n",
" --wandb.enable=true"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"**Notes**:\n",
"\n",
"- If using a local dataset, add `--dataset.root=/path/to/dataset`.\n",
"- Adjust `--batch_size` and `--steps` based on your hardware and dataset.\n",
"- Model checkpoints, logs, and training plots will be saved to the specified `--output_dir`\n",
"- Training progress visualized in your wandb dashboard\n"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Download Models from Hugging Face to Local Machine\n",
"Now after training is done, download the model to local machine. "
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "zFMLGuVkH7UN",
"outputId": "535f0717-4d6b-4eeb-beee-25416f7af383"
},
"outputs": [],
"source": [
"!huggingface-cli download ${HF_USER}/{REPO_NAME} --repo-type model --local-dir path/to/model\n",
"# e.g. huggingface-cli upload ${HF_USER}/act_so101_3cube_1ksteps \\\n",
"# outputs/train/act_so101_3cube_1ksteps/checkpoints/last/pretrained_model"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Miscs\n",
"1. Once the environment is setup, you can open a terminal session for training by navigating to `File → New Launcher → Other → Terminal`.\n",
"2. You can also upload your datasets to the container by clicking the `Upload Files` button in the left pane."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Q&A\n",
"1. If you encounter an error like:\n",
" ```\n",
" FileExistsError: Output directory outputs/train/act_so101_3cube_1ksteps already exists and resume is False. Please change your output directory so that outputs/train/act_so101_3cube_1ksteps is not overwritten. \n",
" ```\n",
" Remove the existing directory before proceeding:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "zFMLGuVkH7UN",
"outputId": "535f0717-4d6b-4eeb-beee-25416f7af383"
},
"outputs": [],
"source": [
"!rm -fr outputs/train/act_so101_3cube_1ksteps"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"2. When running models other than ACT, ensure you install the required additional dependencies for those models."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# For smolVLA\n",
"!cd lerobot && pip install -e \".[smolvla]\"\n",
"# For Pi\n",
"!cd lerobot && pip install -e \".[pi]\""
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"3. If you want to resume the training from last checkpoint, run the command below:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"!lerobot-train \\\n",
" --resume=true \\\n",
" --config_path=outputs/train/<job name>/checkpoints/last/pretrained_model/train_config.json \\\n",
" --steps=<new total steps>"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"4. If you want to upload your dataset using `huggingface-cli upload <repo name> <path to the dataset> --repo-type=dataset`, be sure to set a codebase tag like below:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"from huggingface_hub import HfApi\n",
"from huggingface_hub import login\n",
"\n",
"login(token=\"your_huggingface_token\")\n",
"hub_api = HfApi()\n",
"hub_api.create_tag(<HF_REPO_NAME>, tag=\"v3.0\", revision=\"main\", repo_type=\"dataset\")"
]
}
],
"metadata": {
"accelerator": "GPU",
"colab": {
"gpuType": "A100",
"machine_shape": "hm",
"provenance": []
},
"kernelspec": {
"display_name": "Python 3 (ipykernel)",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.12.3"
}
},
"nbformat": 4,
"nbformat_minor": 4
}
